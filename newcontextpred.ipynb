{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7bde98190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea'), (['the', 'idea', 'a', 'computational'], 'of')]\n",
      "about\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.Many years later, the village was holding a hunting competition. The girl's father, the priest, the prince, and the advisor were all there. The girl recognized them (knew who they were), but they did not recognize her.\n",
    "\n",
    "At the competition, the girl stood up and told her life story to the crowd.\n",
    "\n",
    "Everyone immediately recognized her. They understood she was telling the truth! The girl, the prince, and her father were all happily reunited (brought together again). The priest and advisor were banished to prison.\n",
    "\n",
    "They soon learned that the son who had been stolen was actually living in the village. The girl and her husband, the prince, were then reunited with their son.\n",
    "\n",
    "The prince was so happy that he passed a law that said that hunters could not hunt (kill) gazelles. From that day on, the gazelles were protected from harm.\n",
    "\"\"\".split()\n",
    "\n",
    "\n",
    "vocab=set(raw_text)\n",
    "vocab_size=len(vocab)\n",
    "word2idx={word:i for i,word in enumerate(vocab)}\n",
    "idx2word={i:word for i,word in enumerate(vocab)}\n",
    "#print(word2idx)\n",
    "embedding_size=6\n",
    "\n",
    "\n",
    "data=[]\n",
    "for i in range(2,len(raw_text)-2):\n",
    "    context=[raw_text[i - 2], raw_text[i - 1],raw_text[i + 1], raw_text[i + 2]]\n",
    "    target=raw_text[i]\n",
    "    data.append((context,target))\n",
    "\n",
    "print(data[0:6])\n",
    "\n",
    "def make_context_vector(context,word2idx):\n",
    "    idxs=[word2idx[w] for w in context]\n",
    "    return torch.tensor(idxs,dtype=torch.long)\n",
    "def make_context_vector1(context,word2idx):\n",
    "    idxs=word2idx[context]\n",
    "    return torch.tensor(idxs,dtype=torch.long)\n",
    "\n",
    "\n",
    "a=make_context_vector(data[0][0],word2idx)\n",
    "print (data[0][1]) \n",
    "\n",
    "\n",
    "e=nn.Embedding(vocab_size,embedding_size)\n",
    "a=e(a)\n",
    "#print(a.size())\n",
    "max_n=vocab_size+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,batch_size,output_size,num_layers):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.output_size=output_size\n",
    "        self.input_size=input_size\n",
    "        self.num_layers=num_layers\n",
    "        self.lstm=nn.LSTM(self.input_size,self.hidden_size,self.num_layers)\n",
    "        \n",
    "        self.word_embeddings=nn.Embedding(vocab_size,embedding_size)\n",
    "        \n",
    "        self.linear=nn.Linear(self.hidden_size,embedding_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,inputi,target):\n",
    "        \n",
    "        embedsi=self.word_embeddings(inputi)\n",
    "        \n",
    "        embedso=self.word_embeddings(target)\n",
    "        \n",
    "        out,self.hidden=self.lstm(embedsi.view(len(inputi),1,-1))\n",
    "        #out=torch.randn(1,hidden_size)\n",
    "        \n",
    "        y_pred = self.linear(out[-1].view(1, -1))\n",
    "    \n",
    "        loss=((embedso-y_pred)**2).mean()\n",
    "    \n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(max_n, self.hidden_dim, self.num_layers)\n",
    "        self.word_embeddings=nn.Embedding(vocab_size,embedding_size)\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, max_n)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, inputi,target):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        embedsi=self.word_embeddings(inputi)\n",
    "        embedsi=torch.nn.functional.one_hot(inputi,max_n)\n",
    "        embedso=self.word_embeddings(target)\n",
    "        targett=one_hot = torch.nn.functional.one_hot(target, max_n)\n",
    "        \n",
    "        targett=targett.type(torch.long)\n",
    "        embedsi=embedsi.type(torch.float)\n",
    "        out,self.hidden=self.lstm(embedsi.view(len(inputi),1,-1))\n",
    "        #out=torch.randn(1,self.hidden_dim)\n",
    "        \n",
    "        y_pred = self.linear(out[-1].view(1, -1))\n",
    "\n",
    "        y_pred=F.softmax(y_pred,dim=1)\n",
    "        \n",
    "        targett=targett.view(1,-1)\n",
    "        \n",
    "        return y_pred,targett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0072, 0.0076, 0.0071, 0.0074, 0.0072, 0.0068, 0.0073, 0.0078, 0.0079,\n",
      "         0.0079, 0.0074, 0.0078, 0.0076, 0.0079, 0.0078, 0.0076, 0.0073, 0.0069,\n",
      "         0.0068, 0.0075, 0.0068, 0.0068, 0.0076, 0.0068, 0.0066, 0.0072, 0.0070,\n",
      "         0.0070, 0.0071, 0.0070, 0.0069, 0.0072, 0.0075, 0.0069, 0.0066, 0.0066,\n",
      "         0.0076, 0.0076, 0.0072, 0.0070, 0.0076, 0.0078, 0.0073, 0.0069, 0.0076,\n",
      "         0.0078, 0.0077, 0.0077, 0.0079, 0.0078, 0.0072, 0.0074, 0.0078, 0.0075,\n",
      "         0.0077, 0.0073, 0.0067, 0.0079, 0.0075, 0.0074, 0.0072, 0.0067, 0.0074,\n",
      "         0.0075, 0.0078, 0.0069, 0.0079, 0.0068, 0.0071, 0.0071, 0.0068, 0.0081,\n",
      "         0.0075, 0.0072, 0.0067, 0.0070, 0.0075, 0.0076, 0.0070, 0.0068, 0.0070,\n",
      "         0.0064, 0.0074, 0.0067, 0.0079, 0.0067, 0.0074, 0.0075, 0.0072, 0.0072,\n",
      "         0.0067, 0.0076, 0.0079, 0.0068, 0.0067, 0.0077, 0.0070, 0.0075, 0.0079,\n",
      "         0.0075, 0.0074, 0.0077, 0.0076, 0.0071, 0.0076, 0.0069, 0.0074, 0.0073,\n",
      "         0.0067, 0.0069, 0.0072, 0.0078, 0.0071, 0.0072, 0.0079, 0.0070, 0.0081,\n",
      "         0.0075, 0.0078, 0.0068, 0.0072, 0.0077, 0.0074, 0.0073, 0.0070, 0.0079,\n",
      "         0.0074, 0.0073, 0.0076, 0.0074, 0.0075, 0.0067, 0.0071, 0.0073, 0.0066,\n",
      "         0.0068, 0.0070]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([139.3499], grad_fn=<AddBackward0>)\n",
      "tensor([139.3429], grad_fn=<AddBackward0>)\n",
      "tensor([139.3300], grad_fn=<AddBackward0>)\n",
      "tensor([139.3450], grad_fn=<AddBackward0>)\n",
      "tensor([139.3480], grad_fn=<AddBackward0>)\n",
      "tensor([139.3613], grad_fn=<AddBackward0>)\n",
      "tensor([139.3813], grad_fn=<AddBackward0>)\n",
      "tensor([139.4014], grad_fn=<AddBackward0>)\n",
      "tensor([139.4018], grad_fn=<AddBackward0>)\n",
      "tensor([139.4016], grad_fn=<AddBackward0>)\n",
      "tensor([139.4387], grad_fn=<AddBackward0>)\n",
      "tensor([139.4465], grad_fn=<AddBackward0>)\n",
      "tensor([139.3791], grad_fn=<AddBackward0>)\n",
      "tensor([139.4171], grad_fn=<AddBackward0>)\n",
      "tensor([139.4402], grad_fn=<AddBackward0>)\n",
      "tensor([139.4421], grad_fn=<AddBackward0>)\n",
      "tensor([139.3999], grad_fn=<AddBackward0>)\n",
      "tensor([139.3928], grad_fn=<AddBackward0>)\n",
      "tensor([139.3827], grad_fn=<AddBackward0>)\n",
      "tensor([139.3656], grad_fn=<AddBackward0>)\n",
      "tensor([139.3871], grad_fn=<AddBackward0>)\n",
      "tensor([139.3914], grad_fn=<AddBackward0>)\n",
      "tensor([139.3875], grad_fn=<AddBackward0>)\n",
      "tensor([139.3783], grad_fn=<AddBackward0>)\n",
      "tensor([139.3756], grad_fn=<AddBackward0>)\n",
      "tensor([139.3777], grad_fn=<AddBackward0>)\n",
      "tensor([139.3673], grad_fn=<AddBackward0>)\n",
      "tensor([139.3642], grad_fn=<AddBackward0>)\n",
      "tensor([139.3587], grad_fn=<AddBackward0>)\n",
      "tensor([139.3407], grad_fn=<AddBackward0>)\n",
      "tensor([139.3246], grad_fn=<AddBackward0>)\n",
      "tensor([139.3192], grad_fn=<AddBackward0>)\n",
      "tensor([139.3028], grad_fn=<AddBackward0>)\n",
      "tensor([139.2832], grad_fn=<AddBackward0>)\n",
      "tensor([139.2656], grad_fn=<AddBackward0>)\n",
      "tensor([139.2482], grad_fn=<AddBackward0>)\n",
      "tensor([139.2298], grad_fn=<AddBackward0>)\n",
      "tensor([139.2025], grad_fn=<AddBackward0>)\n",
      "tensor([139.1780], grad_fn=<AddBackward0>)\n",
      "tensor([139.1482], grad_fn=<AddBackward0>)\n",
      "tensor([139.1236], grad_fn=<AddBackward0>)\n",
      "tensor([139.1012], grad_fn=<AddBackward0>)\n",
      "tensor([139.0830], grad_fn=<AddBackward0>)\n",
      "tensor([139.0602], grad_fn=<AddBackward0>)\n",
      "tensor([139.0393], grad_fn=<AddBackward0>)\n",
      "tensor([139.0179], grad_fn=<AddBackward0>)\n",
      "tensor([139.0009], grad_fn=<AddBackward0>)\n",
      "tensor([138.9701], grad_fn=<AddBackward0>)\n",
      "tensor([138.9379], grad_fn=<AddBackward0>)\n",
      "tensor([138.9291], grad_fn=<AddBackward0>)\n",
      "tensor([138.9115], grad_fn=<AddBackward0>)\n",
      "tensor([138.9016], grad_fn=<AddBackward0>)\n",
      "tensor([138.8874], grad_fn=<AddBackward0>)\n",
      "tensor([138.8779], grad_fn=<AddBackward0>)\n",
      "tensor([138.8721], grad_fn=<AddBackward0>)\n",
      "tensor([138.8654], grad_fn=<AddBackward0>)\n",
      "tensor([138.8564], grad_fn=<AddBackward0>)\n",
      "tensor([138.8496], grad_fn=<AddBackward0>)\n",
      "tensor([138.8478], grad_fn=<AddBackward0>)\n",
      "tensor([138.8409], grad_fn=<AddBackward0>)\n",
      "tensor([138.8376], grad_fn=<AddBackward0>)\n",
      "tensor([138.8309], grad_fn=<AddBackward0>)\n",
      "tensor([138.8228], grad_fn=<AddBackward0>)\n",
      "tensor([138.8083], grad_fn=<AddBackward0>)\n",
      "tensor([138.7957], grad_fn=<AddBackward0>)\n",
      "tensor([138.7822], grad_fn=<AddBackward0>)\n",
      "tensor([138.7806], grad_fn=<AddBackward0>)\n",
      "tensor([138.7780], grad_fn=<AddBackward0>)\n",
      "tensor([138.7742], grad_fn=<AddBackward0>)\n",
      "tensor([138.7699], grad_fn=<AddBackward0>)\n",
      "tensor([138.7615], grad_fn=<AddBackward0>)\n",
      "tensor([138.7499], grad_fn=<AddBackward0>)\n",
      "tensor([138.7403], grad_fn=<AddBackward0>)\n",
      "tensor([138.7319], grad_fn=<AddBackward0>)\n",
      "tensor([138.7251], grad_fn=<AddBackward0>)\n",
      "tensor([138.7199], grad_fn=<AddBackward0>)\n",
      "tensor([138.7143], grad_fn=<AddBackward0>)\n",
      "tensor([138.7090], grad_fn=<AddBackward0>)\n",
      "tensor([138.7040], grad_fn=<AddBackward0>)\n",
      "tensor([138.6990], grad_fn=<AddBackward0>)\n",
      "tensor([138.6941], grad_fn=<AddBackward0>)\n",
      "tensor([138.6880], grad_fn=<AddBackward0>)\n",
      "tensor([138.6777], grad_fn=<AddBackward0>)\n",
      "tensor([138.6679], grad_fn=<AddBackward0>)\n",
      "tensor([138.6638], grad_fn=<AddBackward0>)\n",
      "tensor([138.6549], grad_fn=<AddBackward0>)\n",
      "tensor([138.6492], grad_fn=<AddBackward0>)\n",
      "tensor([138.6465], grad_fn=<AddBackward0>)\n",
      "tensor([138.6447], grad_fn=<AddBackward0>)\n",
      "tensor([138.6434], grad_fn=<AddBackward0>)\n",
      "tensor([138.6425], grad_fn=<AddBackward0>)\n",
      "tensor([138.6414], grad_fn=<AddBackward0>)\n",
      "tensor([138.6386], grad_fn=<AddBackward0>)\n",
      "tensor([138.6350], grad_fn=<AddBackward0>)\n",
      "tensor([138.6289], grad_fn=<AddBackward0>)\n",
      "tensor([138.6234], grad_fn=<AddBackward0>)\n",
      "tensor([138.6189], grad_fn=<AddBackward0>)\n",
      "tensor([138.6088], grad_fn=<AddBackward0>)\n",
      "tensor([138.5981], grad_fn=<AddBackward0>)\n",
      "tensor([138.5897], grad_fn=<AddBackward0>)\n",
      "tensor([138.5828], grad_fn=<AddBackward0>)\n",
      "tensor([138.5805], grad_fn=<AddBackward0>)\n",
      "tensor([138.5750], grad_fn=<AddBackward0>)\n",
      "tensor([138.5672], grad_fn=<AddBackward0>)\n",
      "tensor([138.5571], grad_fn=<AddBackward0>)\n",
      "tensor([138.5507], grad_fn=<AddBackward0>)\n",
      "tensor([138.5455], grad_fn=<AddBackward0>)\n",
      "tensor([138.5416], grad_fn=<AddBackward0>)\n",
      "tensor([138.5395], grad_fn=<AddBackward0>)\n",
      "tensor([138.5353], grad_fn=<AddBackward0>)\n",
      "tensor([138.5336], grad_fn=<AddBackward0>)\n",
      "tensor([138.5319], grad_fn=<AddBackward0>)\n",
      "tensor([138.5241], grad_fn=<AddBackward0>)\n",
      "tensor([138.5179], grad_fn=<AddBackward0>)\n",
      "tensor([138.5091], grad_fn=<AddBackward0>)\n",
      "tensor([138.5075], grad_fn=<AddBackward0>)\n",
      "tensor([138.5067], grad_fn=<AddBackward0>)\n",
      "tensor([138.5033], grad_fn=<AddBackward0>)\n",
      "tensor([138.4972], grad_fn=<AddBackward0>)\n",
      "tensor([138.4912], grad_fn=<AddBackward0>)\n",
      "tensor([138.4854], grad_fn=<AddBackward0>)\n",
      "tensor([138.4805], grad_fn=<AddBackward0>)\n",
      "tensor([138.4749], grad_fn=<AddBackward0>)\n",
      "tensor([138.4647], grad_fn=<AddBackward0>)\n",
      "tensor([138.4605], grad_fn=<AddBackward0>)\n",
      "tensor([138.4553], grad_fn=<AddBackward0>)\n",
      "tensor([138.4535], grad_fn=<AddBackward0>)\n",
      "tensor([138.4488], grad_fn=<AddBackward0>)\n",
      "tensor([138.4423], grad_fn=<AddBackward0>)\n",
      "tensor([138.4405], grad_fn=<AddBackward0>)\n",
      "tensor([138.4376], grad_fn=<AddBackward0>)\n",
      "tensor([138.4263], grad_fn=<AddBackward0>)\n",
      "tensor([138.4163], grad_fn=<AddBackward0>)\n",
      "tensor([138.4131], grad_fn=<AddBackward0>)\n",
      "tensor([138.4101], grad_fn=<AddBackward0>)\n",
      "tensor([138.4012], grad_fn=<AddBackward0>)\n",
      "tensor([138.3970], grad_fn=<AddBackward0>)\n",
      "tensor([138.3879], grad_fn=<AddBackward0>)\n",
      "tensor([138.3770], grad_fn=<AddBackward0>)\n",
      "tensor([138.3651], grad_fn=<AddBackward0>)\n",
      "tensor([138.3645], grad_fn=<AddBackward0>)\n",
      "tensor([138.3630], grad_fn=<AddBackward0>)\n",
      "tensor([138.3558], grad_fn=<AddBackward0>)\n",
      "tensor([138.3555], grad_fn=<AddBackward0>)\n",
      "tensor([138.3539], grad_fn=<AddBackward0>)\n",
      "tensor([138.3512], grad_fn=<AddBackward0>)\n",
      "tensor([138.3502], grad_fn=<AddBackward0>)\n",
      "tensor([138.3460], grad_fn=<AddBackward0>)\n",
      "tensor([138.3385], grad_fn=<AddBackward0>)\n",
      "tensor([138.3316], grad_fn=<AddBackward0>)\n",
      "tensor([138.3258], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([138.3260], grad_fn=<AddBackward0>)\n",
      "tensor([138.3242], grad_fn=<AddBackward0>)\n",
      "tensor([138.3190], grad_fn=<AddBackward0>)\n",
      "tensor([138.3177], grad_fn=<AddBackward0>)\n",
      "tensor([138.3175], grad_fn=<AddBackward0>)\n",
      "tensor([138.3174], grad_fn=<AddBackward0>)\n",
      "tensor([138.3172], grad_fn=<AddBackward0>)\n",
      "tensor([138.3172], grad_fn=<AddBackward0>)\n",
      "tensor([138.3170], grad_fn=<AddBackward0>)\n",
      "tensor([138.3170], grad_fn=<AddBackward0>)\n",
      "tensor([138.3169], grad_fn=<AddBackward0>)\n",
      "tensor([138.3168], grad_fn=<AddBackward0>)\n",
      "tensor([138.3156], grad_fn=<AddBackward0>)\n",
      "tensor([138.3106], grad_fn=<AddBackward0>)\n",
      "tensor([138.3111], grad_fn=<AddBackward0>)\n",
      "tensor([138.3106], grad_fn=<AddBackward0>)\n",
      "tensor([138.3106], grad_fn=<AddBackward0>)\n",
      "tensor([138.3105], grad_fn=<AddBackward0>)\n",
      "tensor([138.3104], grad_fn=<AddBackward0>)\n",
      "tensor([138.3102], grad_fn=<AddBackward0>)\n",
      "tensor([138.3048], grad_fn=<AddBackward0>)\n",
      "tensor([138.3042], grad_fn=<AddBackward0>)\n",
      "tensor([138.3040], grad_fn=<AddBackward0>)\n",
      "tensor([138.3040], grad_fn=<AddBackward0>)\n",
      "tensor([138.3035], grad_fn=<AddBackward0>)\n",
      "tensor([138.3038], grad_fn=<AddBackward0>)\n",
      "tensor([138.3038], grad_fn=<AddBackward0>)\n",
      "tensor([138.3038], grad_fn=<AddBackward0>)\n",
      "tensor([138.3038], grad_fn=<AddBackward0>)\n",
      "tensor([138.3038], grad_fn=<AddBackward0>)\n",
      "tensor([138.3037], grad_fn=<AddBackward0>)\n",
      "tensor([138.3034], grad_fn=<AddBackward0>)\n",
      "tensor([138.3000], grad_fn=<AddBackward0>)\n",
      "tensor([138.2972], grad_fn=<AddBackward0>)\n",
      "tensor([138.2948], grad_fn=<AddBackward0>)\n",
      "tensor([138.2933], grad_fn=<AddBackward0>)\n",
      "tensor([138.2918], grad_fn=<AddBackward0>)\n",
      "tensor([138.2921], grad_fn=<AddBackward0>)\n",
      "tensor([138.2916], grad_fn=<AddBackward0>)\n",
      "tensor([138.2863], grad_fn=<AddBackward0>)\n",
      "tensor([138.2853], grad_fn=<AddBackward0>)\n",
      "tensor([138.2862], grad_fn=<AddBackward0>)\n",
      "tensor([138.2853], grad_fn=<AddBackward0>)\n",
      "tensor([138.2852], grad_fn=<AddBackward0>)\n",
      "tensor([138.2849], grad_fn=<AddBackward0>)\n",
      "tensor([138.2843], grad_fn=<AddBackward0>)\n",
      "tensor([138.2799], grad_fn=<AddBackward0>)\n",
      "tensor([138.2779], grad_fn=<AddBackward0>)\n",
      "tensor([138.2753], grad_fn=<AddBackward0>)\n",
      "tensor([138.2721], grad_fn=<AddBackward0>)\n",
      "tensor([138.2717], grad_fn=<AddBackward0>)\n",
      "tensor([138.2717], grad_fn=<AddBackward0>)\n",
      "tensor([138.2716], grad_fn=<AddBackward0>)\n",
      "tensor([138.2710], grad_fn=<AddBackward0>)\n",
      "tensor([138.2654], grad_fn=<AddBackward0>)\n",
      "tensor([138.2655], grad_fn=<AddBackward0>)\n",
      "tensor([138.2651], grad_fn=<AddBackward0>)\n",
      "tensor([138.2651], grad_fn=<AddBackward0>)\n",
      "tensor([138.2651], grad_fn=<AddBackward0>)\n",
      "tensor([138.2650], grad_fn=<AddBackward0>)\n",
      "tensor([138.2649], grad_fn=<AddBackward0>)\n",
      "tensor([138.2649], grad_fn=<AddBackward0>)\n",
      "tensor([138.2648], grad_fn=<AddBackward0>)\n",
      "tensor([138.2648], grad_fn=<AddBackward0>)\n",
      "tensor([138.2647], grad_fn=<AddBackward0>)\n",
      "tensor([138.2643], grad_fn=<AddBackward0>)\n",
      "tensor([138.2603], grad_fn=<AddBackward0>)\n",
      "tensor([138.2584], grad_fn=<AddBackward0>)\n",
      "tensor([138.2585], grad_fn=<AddBackward0>)\n",
      "tensor([138.2583], grad_fn=<AddBackward0>)\n",
      "tensor([138.2583], grad_fn=<AddBackward0>)\n",
      "tensor([138.2580], grad_fn=<AddBackward0>)\n",
      "tensor([138.2557], grad_fn=<AddBackward0>)\n",
      "tensor([138.2520], grad_fn=<AddBackward0>)\n",
      "tensor([138.2513], grad_fn=<AddBackward0>)\n",
      "tensor([138.2467], grad_fn=<AddBackward0>)\n",
      "tensor([138.2455], grad_fn=<AddBackward0>)\n",
      "tensor([138.2455], grad_fn=<AddBackward0>)\n",
      "tensor([138.2454], grad_fn=<AddBackward0>)\n",
      "tensor([138.2454], grad_fn=<AddBackward0>)\n",
      "tensor([138.2460], grad_fn=<AddBackward0>)\n",
      "tensor([138.2455], grad_fn=<AddBackward0>)\n",
      "tensor([138.2459], grad_fn=<AddBackward0>)\n",
      "tensor([138.2457], grad_fn=<AddBackward0>)\n",
      "tensor([138.2455], grad_fn=<AddBackward0>)\n",
      "tensor([138.2451], grad_fn=<AddBackward0>)\n",
      "tensor([138.2401], grad_fn=<AddBackward0>)\n",
      "tensor([138.2389], grad_fn=<AddBackward0>)\n",
      "tensor([138.2390], grad_fn=<AddBackward0>)\n",
      "tensor([138.2390], grad_fn=<AddBackward0>)\n",
      "tensor([138.2390], grad_fn=<AddBackward0>)\n",
      "tensor([138.2389], grad_fn=<AddBackward0>)\n",
      "tensor([138.2389], grad_fn=<AddBackward0>)\n",
      "tensor([138.2388], grad_fn=<AddBackward0>)\n",
      "tensor([138.2388], grad_fn=<AddBackward0>)\n",
      "tensor([138.2388], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2387], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2386], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2385], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2384], grad_fn=<AddBackward0>)\n",
      "tensor([138.2364], grad_fn=<AddBackward0>)\n",
      "tensor([138.2327], grad_fn=<AddBackward0>)\n",
      "tensor([138.2328], grad_fn=<AddBackward0>)\n",
      "tensor([138.2327], grad_fn=<AddBackward0>)\n",
      "tensor([138.2329], grad_fn=<AddBackward0>)\n",
      "tensor([138.2327], grad_fn=<AddBackward0>)\n",
      "tensor([138.2328], grad_fn=<AddBackward0>)\n",
      "tensor([138.2326], grad_fn=<AddBackward0>)\n",
      "tensor([138.2327], grad_fn=<AddBackward0>)\n",
      "tensor([138.2287], grad_fn=<AddBackward0>)\n",
      "tensor([138.2267], grad_fn=<AddBackward0>)\n",
      "tensor([138.2259], grad_fn=<AddBackward0>)\n",
      "tensor([138.2263], grad_fn=<AddBackward0>)\n",
      "tensor([138.2261], grad_fn=<AddBackward0>)\n",
      "tensor([138.2261], grad_fn=<AddBackward0>)\n",
      "tensor([138.2260], grad_fn=<AddBackward0>)\n",
      "tensor([138.2260], grad_fn=<AddBackward0>)\n",
      "tensor([138.2261], grad_fn=<AddBackward0>)\n",
      "tensor([138.2259], grad_fn=<AddBackward0>)\n",
      "tensor([138.2259], grad_fn=<AddBackward0>)\n",
      "tensor([138.2258], grad_fn=<AddBackward0>)\n",
      "tensor([138.2258], grad_fn=<AddBackward0>)\n",
      "tensor([138.2262], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2259], grad_fn=<AddBackward0>)\n",
      "tensor([138.2260], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2258], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2257], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2256], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2255], grad_fn=<AddBackward0>)\n",
      "tensor([138.2225], grad_fn=<AddBackward0>)\n",
      "tensor([138.2247], grad_fn=<AddBackward0>)\n",
      "tensor([138.2205], grad_fn=<AddBackward0>)\n",
      "tensor([138.2489], grad_fn=<AddBackward0>)\n",
      "tensor([138.2591], grad_fn=<AddBackward0>)\n",
      "tensor([138.2714], grad_fn=<AddBackward0>)\n",
      "tensor([138.2858], grad_fn=<AddBackward0>)\n",
      "tensor([138.2971], grad_fn=<AddBackward0>)\n",
      "tensor([138.2667], grad_fn=<AddBackward0>)\n",
      "tensor([138.2847], grad_fn=<AddBackward0>)\n",
      "tensor([138.3032], grad_fn=<AddBackward0>)\n",
      "tensor([138.2950], grad_fn=<AddBackward0>)\n",
      "tensor([138.2883], grad_fn=<AddBackward0>)\n",
      "tensor([138.2898], grad_fn=<AddBackward0>)\n",
      "tensor([138.3031], grad_fn=<AddBackward0>)\n",
      "tensor([138.2918], grad_fn=<AddBackward0>)\n",
      "tensor([138.3068], grad_fn=<AddBackward0>)\n",
      "tensor([138.3064], grad_fn=<AddBackward0>)\n",
      "tensor([138.3001], grad_fn=<AddBackward0>)\n",
      "tensor([138.2810], grad_fn=<AddBackward0>)\n",
      "tensor([138.2717], grad_fn=<AddBackward0>)\n",
      "tensor([138.2634], grad_fn=<AddBackward0>)\n",
      "tensor([138.2651], grad_fn=<AddBackward0>)\n",
      "tensor([138.2620], grad_fn=<AddBackward0>)\n",
      "tensor([138.2553], grad_fn=<AddBackward0>)\n",
      "tensor([138.2540], grad_fn=<AddBackward0>)\n",
      "tensor([138.2545], grad_fn=<AddBackward0>)\n",
      "tensor([138.2536], grad_fn=<AddBackward0>)\n",
      "tensor([138.2673], grad_fn=<AddBackward0>)\n",
      "tensor([138.2554], grad_fn=<AddBackward0>)\n",
      "tensor([138.2604], grad_fn=<AddBackward0>)\n",
      "tensor([138.2547], grad_fn=<AddBackward0>)\n",
      "tensor([138.2460], grad_fn=<AddBackward0>)\n",
      "tensor([138.2466], grad_fn=<AddBackward0>)\n",
      "tensor([138.2483], grad_fn=<AddBackward0>)\n",
      "tensor([138.2467], grad_fn=<AddBackward0>)\n",
      "tensor([138.2463], grad_fn=<AddBackward0>)\n",
      "tensor([138.2461], grad_fn=<AddBackward0>)\n",
      "tensor([138.2466], grad_fn=<AddBackward0>)\n",
      "tensor([138.2457], grad_fn=<AddBackward0>)\n",
      "tensor([138.2446], grad_fn=<AddBackward0>)\n",
      "tensor([138.2414], grad_fn=<AddBackward0>)\n",
      "tensor([138.2397], grad_fn=<AddBackward0>)\n",
      "tensor([138.2389], grad_fn=<AddBackward0>)\n",
      "tensor([138.2392], grad_fn=<AddBackward0>)\n",
      "tensor([138.2379], grad_fn=<AddBackward0>)\n",
      "tensor([138.2327], grad_fn=<AddBackward0>)\n",
      "tensor([138.2320], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([138.2321], grad_fn=<AddBackward0>)\n",
      "tensor([138.2319], grad_fn=<AddBackward0>)\n",
      "tensor([138.2318], grad_fn=<AddBackward0>)\n",
      "tensor([138.2311], grad_fn=<AddBackward0>)\n",
      "tensor([138.2317], grad_fn=<AddBackward0>)\n",
      "tensor([138.2315], grad_fn=<AddBackward0>)\n",
      "tensor([138.2311], grad_fn=<AddBackward0>)\n",
      "tensor([138.2313], grad_fn=<AddBackward0>)\n",
      "tensor([138.2311], grad_fn=<AddBackward0>)\n",
      "tensor([138.2311], grad_fn=<AddBackward0>)\n",
      "tensor([138.2307], grad_fn=<AddBackward0>)\n",
      "tensor([138.2307], grad_fn=<AddBackward0>)\n",
      "tensor([138.2309], grad_fn=<AddBackward0>)\n",
      "tensor([138.2307], grad_fn=<AddBackward0>)\n",
      "tensor([138.2308], grad_fn=<AddBackward0>)\n",
      "tensor([138.2306], grad_fn=<AddBackward0>)\n",
      "tensor([138.2308], grad_fn=<AddBackward0>)\n",
      "tensor([138.2307], grad_fn=<AddBackward0>)\n",
      "tensor([138.2306], grad_fn=<AddBackward0>)\n",
      "tensor([138.2301], grad_fn=<AddBackward0>)\n",
      "tensor([138.2303], grad_fn=<AddBackward0>)\n",
      "tensor([138.2301], grad_fn=<AddBackward0>)\n",
      "tensor([138.2300], grad_fn=<AddBackward0>)\n",
      "tensor([138.2299], grad_fn=<AddBackward0>)\n",
      "tensor([138.2299], grad_fn=<AddBackward0>)\n",
      "tensor([138.2297], grad_fn=<AddBackward0>)\n",
      "tensor([138.2287], grad_fn=<AddBackward0>)\n",
      "tensor([138.2245], grad_fn=<AddBackward0>)\n",
      "tensor([138.2234], grad_fn=<AddBackward0>)\n",
      "tensor([138.2232], grad_fn=<AddBackward0>)\n",
      "tensor([138.2232], grad_fn=<AddBackward0>)\n",
      "tensor([138.2232], grad_fn=<AddBackward0>)\n",
      "tensor([138.2231], grad_fn=<AddBackward0>)\n",
      "tensor([138.2231], grad_fn=<AddBackward0>)\n",
      "tensor([138.2230], grad_fn=<AddBackward0>)\n",
      "tensor([138.2230], grad_fn=<AddBackward0>)\n",
      "tensor([138.2230], grad_fn=<AddBackward0>)\n",
      "tensor([138.2229], grad_fn=<AddBackward0>)\n",
      "tensor([138.2222], grad_fn=<AddBackward0>)\n",
      "tensor([138.2183], grad_fn=<AddBackward0>)\n",
      "tensor([138.2165], grad_fn=<AddBackward0>)\n",
      "tensor([138.2169], grad_fn=<AddBackward0>)\n",
      "tensor([138.2167], grad_fn=<AddBackward0>)\n",
      "tensor([138.2165], grad_fn=<AddBackward0>)\n",
      "tensor([138.2165], grad_fn=<AddBackward0>)\n",
      "tensor([138.2165], grad_fn=<AddBackward0>)\n",
      "tensor([138.2165], grad_fn=<AddBackward0>)\n",
      "tensor([138.2164], grad_fn=<AddBackward0>)\n",
      "tensor([138.2164], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2160], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2163], grad_fn=<AddBackward0>)\n",
      "tensor([138.2162], grad_fn=<AddBackward0>)\n",
      "tensor([138.2157], grad_fn=<AddBackward0>)\n",
      "tensor([138.2156], grad_fn=<AddBackward0>)\n",
      "tensor([138.2156], grad_fn=<AddBackward0>)\n",
      "tensor([138.2156], grad_fn=<AddBackward0>)\n",
      "tensor([138.2154], grad_fn=<AddBackward0>)\n",
      "tensor([138.2153], grad_fn=<AddBackward0>)\n",
      "tensor([138.2153], grad_fn=<AddBackward0>)\n",
      "tensor([138.2146], grad_fn=<AddBackward0>)\n",
      "tensor([138.2148], grad_fn=<AddBackward0>)\n",
      "tensor([138.2152], grad_fn=<AddBackward0>)\n",
      "tensor([138.2145], grad_fn=<AddBackward0>)\n",
      "tensor([138.2191], grad_fn=<AddBackward0>)\n",
      "tensor([138.2106], grad_fn=<AddBackward0>)\n",
      "tensor([138.2090], grad_fn=<AddBackward0>)\n",
      "tensor([138.2102], grad_fn=<AddBackward0>)\n",
      "tensor([138.2096], grad_fn=<AddBackward0>)\n",
      "tensor([138.2098], grad_fn=<AddBackward0>)\n",
      "tensor([138.2097], grad_fn=<AddBackward0>)\n",
      "tensor([138.2096], grad_fn=<AddBackward0>)\n",
      "tensor([138.2068], grad_fn=<AddBackward0>)\n",
      "tensor([138.2032], grad_fn=<AddBackward0>)\n",
      "tensor([138.2035], grad_fn=<AddBackward0>)\n",
      "tensor([138.2030], grad_fn=<AddBackward0>)\n",
      "tensor([138.2030], grad_fn=<AddBackward0>)\n",
      "tensor([138.2030], grad_fn=<AddBackward0>)\n",
      "tensor([138.2030], grad_fn=<AddBackward0>)\n",
      "tensor([138.2028], grad_fn=<AddBackward0>)\n",
      "tensor([138.2027], grad_fn=<AddBackward0>)\n",
      "tensor([138.2027], grad_fn=<AddBackward0>)\n",
      "tensor([138.2025], grad_fn=<AddBackward0>)\n",
      "tensor([138.2024], grad_fn=<AddBackward0>)\n",
      "tensor([138.2024], grad_fn=<AddBackward0>)\n",
      "tensor([138.2024], grad_fn=<AddBackward0>)\n",
      "tensor([138.2025], grad_fn=<AddBackward0>)\n",
      "tensor([138.2023], grad_fn=<AddBackward0>)\n",
      "tensor([138.2023], grad_fn=<AddBackward0>)\n",
      "tensor([138.2022], grad_fn=<AddBackward0>)\n",
      "tensor([138.2018], grad_fn=<AddBackward0>)\n",
      "tensor([138.2005], grad_fn=<AddBackward0>)\n",
      "tensor([138.1965], grad_fn=<AddBackward0>)\n",
      "tensor([138.1961], grad_fn=<AddBackward0>)\n",
      "tensor([138.1961], grad_fn=<AddBackward0>)\n",
      "tensor([138.1960], grad_fn=<AddBackward0>)\n",
      "tensor([138.1957], grad_fn=<AddBackward0>)\n",
      "tensor([138.1959], grad_fn=<AddBackward0>)\n",
      "tensor([138.1958], grad_fn=<AddBackward0>)\n",
      "tensor([138.1958], grad_fn=<AddBackward0>)\n",
      "tensor([138.1958], grad_fn=<AddBackward0>)\n",
      "tensor([138.1956], grad_fn=<AddBackward0>)\n",
      "tensor([138.1957], grad_fn=<AddBackward0>)\n",
      "tensor([138.1956], grad_fn=<AddBackward0>)\n",
      "tensor([138.1956], grad_fn=<AddBackward0>)\n",
      "tensor([138.1953], grad_fn=<AddBackward0>)\n",
      "tensor([138.1955], grad_fn=<AddBackward0>)\n",
      "tensor([138.1956], grad_fn=<AddBackward0>)\n",
      "tensor([138.1955], grad_fn=<AddBackward0>)\n",
      "tensor([138.1956], grad_fn=<AddBackward0>)\n",
      "tensor([138.1955], grad_fn=<AddBackward0>)\n",
      "tensor([138.1955], grad_fn=<AddBackward0>)\n",
      "tensor([138.1955], grad_fn=<AddBackward0>)\n",
      "tensor([138.1953], grad_fn=<AddBackward0>)\n",
      "tensor([138.1934], grad_fn=<AddBackward0>)\n",
      "tensor([138.1894], grad_fn=<AddBackward0>)\n",
      "tensor([138.1891], grad_fn=<AddBackward0>)\n",
      "tensor([138.1890], grad_fn=<AddBackward0>)\n",
      "tensor([138.1890], grad_fn=<AddBackward0>)\n",
      "tensor([138.1890], grad_fn=<AddBackward0>)\n",
      "tensor([138.1889], grad_fn=<AddBackward0>)\n",
      "tensor([138.1885], grad_fn=<AddBackward0>)\n",
      "tensor([138.1884], grad_fn=<AddBackward0>)\n",
      "tensor([138.1884], grad_fn=<AddBackward0>)\n",
      "tensor([138.1884], grad_fn=<AddBackward0>)\n",
      "tensor([138.1883], grad_fn=<AddBackward0>)\n",
      "tensor([138.1883], grad_fn=<AddBackward0>)\n",
      "tensor([138.1882], grad_fn=<AddBackward0>)\n",
      "tensor([138.1881], grad_fn=<AddBackward0>)\n",
      "tensor([138.1881], grad_fn=<AddBackward0>)\n",
      "tensor([138.1880], grad_fn=<AddBackward0>)\n",
      "tensor([138.1880], grad_fn=<AddBackward0>)\n",
      "tensor([138.1881], grad_fn=<AddBackward0>)\n",
      "tensor([138.1875], grad_fn=<AddBackward0>)\n",
      "tensor([138.1869], grad_fn=<AddBackward0>)\n",
      "tensor([138.1831], grad_fn=<AddBackward0>)\n",
      "tensor([138.1816], grad_fn=<AddBackward0>)\n",
      "tensor([138.1817], grad_fn=<AddBackward0>)\n",
      "tensor([138.1815], grad_fn=<AddBackward0>)\n",
      "tensor([138.1785], grad_fn=<AddBackward0>)\n",
      "tensor([138.1754], grad_fn=<AddBackward0>)\n",
      "tensor([138.1752], grad_fn=<AddBackward0>)\n",
      "tensor([138.1751], grad_fn=<AddBackward0>)\n",
      "tensor([138.1753], grad_fn=<AddBackward0>)\n",
      "tensor([138.1751], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "model=LSTM(max_n,max_n,1,max_n,1)\n",
    "lossfunc=nn.NLLLoss()\n",
    "optimiser=torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "with torch.no_grad():\n",
    "    inputii=make_context_vector(data[0][0],word2idx)\n",
    "    target=make_context_vector1(data[0][1],word2idx)\n",
    "    #print(target)\n",
    "    a,b=model(inputii,target)\n",
    "    print(a)\n",
    "    print(b)\n",
    "\n",
    "for iters in range(1,15):    \n",
    "    for epoch in range (1,epochs):\n",
    "        \n",
    "        \n",
    "        loss=torch.tensor([0])\n",
    "        optimiser.zero_grad()\n",
    "        model.zero_grad()\n",
    "        for a,b in data:\n",
    "            \n",
    "            loss=loss.type(torch.float)\n",
    "            \n",
    "            inputii=make_context_vector(a,word2idx)\n",
    "            targett=make_context_vector1(b,word2idx)\n",
    "            a,b=model(inputii,targett)\n",
    "            \n",
    "            a=a.type(torch.float)\n",
    "            b=b.type(torch.float)\n",
    "            #print(torch.sum(torch.mul(a,b),dim=1))\n",
    "            \n",
    "            \n",
    "            #loss=loss + torch.sum(torch.mul(a,b),dim=1)\n",
    "            loss=loss+nn.BCEWithLogitsLoss()(a,b)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        print(loss)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect,\n",
      "tensor([52]) tensor([37])\n",
      "and\n",
      "effect,\n"
     ]
    }
   ],
   "source": [
    "#print(data[3][0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputii=make_context_vector(['the', 'prince', 'with', 'her'],word2idx)\n",
    "    target=make_context_vector1(data[49][1],word2idx)\n",
    "    #print(target)\n",
    "    print(data[49][1])\n",
    "    a,b=model(inputii,target)\n",
    "    #print(a.size())\n",
    "    #print(b.size())\n",
    "    #print(data[1][0])\n",
    "    _,c=torch.max(a,1)\n",
    "    _,d=torch.max(b,1)\n",
    "    print(c,d)\n",
    "    \n",
    "    print(idx2word[52])\n",
    "    print(idx2word[37])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputii=make_context_vector(data[0][0],word2idx)\n",
    "    target=make_context_vector1(data[0][1],word2idx)\n",
    "    print(target)\n",
    "    #loss=model(inputii,target)\n",
    "    #print(loss)\n",
    "    \n",
    "    \n",
    "targett=one_hot = torch.nn.functional.one_hot(inputii, max_n)\n",
    "#print(targett)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
